#!/usr/bin/env python3

import json
import math
import networkx as nx
import numpy as np
import os
import pandas as pd
import derived_task_definition as td # NOTA BENE!!!
import time


def blocking(task, ds, gold):
    # Initialize a list containing all the blocks
    blocks = list()
    if task.blocking:
        # With blocking, read the blocks from the dedicated file
        with open(task.blocks_path, 'r') as input_file:
            blocks = json.load(input_file)
    else:
        # Without blocking, insert a single list containing all ids
        blocks.append(ds['id'].tolist())

    # If batch version is required, generate the set of candidate pairs performing Cartesian product inside each block
    candidates = set()
    if task.batch:
        for block in blocks:
            for i in block:
                for j in block:
                    if i < j:
                        candidates.add((i, j))
                    if i > j:
                        candidates.add((j, i))
        print("Number of candidate pairs generated by blocking: " + str(len(candidates)) + '\n',
              file=open(task.query_output, "a"))

        # Measure precision and recall of the blocking method (true positives: intersection between candidates and gold)
        tp = gold.intersection(candidates)
        print("Quality of the blocking:", file=open(task.query_output, "a"))
        print("TP: " + str(len(tp)) + ", FP: " + str(len(gold) - len(tp)) + " => R: " + str(
            len(tp) / len(gold)) + ", P: " + str(len(tp) / len(candidates)) + '\n', file=open(task.query_output, "a"))

    # Read from the apposite files the blocks in which a record appears and the cost of each block
    with open(task.record_blocks, 'r') as input_file:
        record_blocks = json.load(input_file)
    with open(task.block_costs, 'r') as input_file:
        block_costs = json.load(input_file)

    return blocks, candidates, record_blocks, block_costs


def resolution(ds, cluster, aggregations):
    # Find the matching elements (contained in the cluster cc) in the dataset
    matches = ds.loc[ds['id'].isin(cluster)]

    # Create the entity (as a dictionary) according to the specified aggregation functions
    entity = dict()
    for item in aggregations.items():
        if item[1] == 'min':
            entity[item[0]] = matches[item[0]].min()
        elif item[1] == 'max':
            entity[item[0]] = matches[item[0]].max()
        elif item[1] == 'avg':
            entity[item[0]] = round(matches[item[0]].mean(), 2)
        elif item[1] == 'sum':
            entity[item[0]] = round(matches[item[0]].sum(), 2)
        elif item[1] == 'vote':
            try:
                entity[item[0]] = matches[item[0]].mode(dropna=False).iloc[0]
            except ValueError:
                entity[item[0]] = matches[item[0]].min()
        elif item[1] == 'random':
            entity[item[0]] = np.random.choice(matches[item[0]])
        elif item[1] == 'concat':
            entity[item[0]] = ' ; '.join(matches[item[0]])

    return entity


def batch_er(task, ds, candidates, gold):
    print("BATCH ENTITY RESOLUTION ALGORITHM\n", file=open(task.query_output, "a"))

    # Create an empty graph
    graph = nx.Graph()

    # Apply matching function on candidate pairs (verify their presence in gold) and add matches to the graph as edges
    for match in gold.intersection(candidates):
        graph.add_edge(match[0], match[1])

    # Detect clusters (connected components) as sets of nodes and call resolution function on them
    entities = list()
    for cluster in nx.connected_components(graph):
        entities.append(resolution(ds, cluster, task.aggregations))

    # Create a new dataset without duplicates
    duplicates = ds.loc[ds['id'].isin(list(graph.nodes))]
    ds = pd.concat([ds, duplicates], ignore_index=True).drop_duplicates(subset=['id'], keep=False)

    # Return the clean dataset obtained by replacing the removed duplicates with the solved entities
    return pd.concat([ds, pd.DataFrame(entities)], ignore_index=True).drop_duplicates(subset=['id'], keep=False)


def brewer(mode, task, ds, gold, blocks, record_blocks, block_costs):
    start_time = time.time()
    if mode == 'lazy':
        print("\nLAZY BREWER\n", file=open(task.query_output, "a"))
    else:
        print("\nEAGER BREWER\n", file=open(task.query_output, "a"))

    # Create the ordering list OL
    ol = list()

    # Keep a list of already solved records (entities, no need for ER)
    done = list()

    # Perform the selection of the seed records (records to be inserted in OL) on each block
    log_file = "results/log" + str(task.counter) + "_" + mode + ".txt"
    start_filtering = time.time()
    # print("start blocking filtering: " + str(start_filtering), file=open(log_file, "a"))
    for block in blocks:
        if len(block) == 1:
            solved = True
        else:
            solved = False
        block_records = ds.loc[ds['id'].isin(block)]
        # Perform preliminary filtering on the records of the block
        seed_records = task.brewer_pre_filtering(block_records, solved)
        no_seed_records = pd.concat([block_records, seed_records], ignore_index=True).drop_duplicates(subset=['id'],
                                                                                                      keep=False)
        # If the block overcomes the filtering (i.e., the list of seed records is not empty)...
        if len(seed_records.index) > 0:
            # ...for Lazy BrewER, insert in OL each record that survives the filtering (seed record)
            if mode == 'lazy':
                # If 'ignore null' option is set:
                extreme_ok = pd.NA
                if task.ignore_null:
                    # First, check that at least one record in the block has a not null ordering key value
                    block_records = block_records[block_records[task.ordering_key].notnull()]
                    # If this is true, select the extreme (worst-case) ordering key value for the emitted entity
                    if len(block_records.index) > 0:
                        if task.ordering_mode == 'asc':
                            extreme_ok = block_records[task.ordering_key].min()
                        else:
                            extreme_ok = block_records[task.ordering_key].max()
                    else:
                        # If this is false, actual seed records must be ignored
                        seed_records = seed_records[seed_records[task.ordering_key].notnull()]
                for index, row in seed_records.iterrows():
                    # Element to be inserted in OL
                    element = dict()
                    # Its attribute 'id' is a list of identifiers, containing at the moment only the one of the record
                    element['id'] = [row['id']]
                    # Its attribute 'ordering_key' must be a numeric value (forced cast to float)
                    # If 'ignore null' option is set, substitute the null OK value using the extreme one
                    if task.ignore_null and pd.isna(float(row[task.ordering_key])):
                        element['ordering_key'] = extreme_ok
                    else:
                        element['ordering_key'] = float(row[task.ordering_key])
                    element['solved'] = solved
                    element['neighbours'] = {'seed': seed_records['id'].tolist(),
                                             'no_seed': no_seed_records['id'].tolist()}
                    if element['id'][0] in element['neighbours']['seed']:
                        element['seed'] = True
                    else:
                        element['seed'] = False
                    ol.append(element)
            # ...for Eager BrewER, insert in OL the whole block
            else:
                # If 'ignore null' option is set, insert the records with null ordering key only as neighbours
                if task.ignore_null:
                    block_records = block_records[block_records[task.ordering_key].notnull()]
                for index, row in block_records.iterrows():
                    element = dict()
                    element['id'] = [row['id']]
                    element['ordering_key'] = float(row[task.ordering_key])
                    element['solved'] = solved
                    element['neighbours'] = {'seed': seed_records['id'].tolist(),
                                             'no_seed': no_seed_records['id'].tolist()}
                    if element['id'][0] in element['neighbours']['seed']:
                        element['seed'] = True
                    else:
                        element['seed'] = False
                    ol.append(element)
    end_filtering = time.time() - start_filtering
    print("blocking filtering: " + str(round(end_filtering * 1000, 2)), file=open(log_file, "a"))

    print("Number of elements inserted in the ordering list: " + str(len(ol)) + '\n', file=open(task.query_output, "a"))

    # In lazy case (for seed records), compute the number of comparisons to be performed without transitive closure
    start_computation = time.time()
    # print("start computation of no-blocking comparisons: " + str(start_computation), file=open(log_file, "a"))
    if mode == 'lazy':
        considered_blocks = set()
        comparisons_without_closure = 0
        for record in ol:
            for block in record_blocks[str(record['id'][0])]:
                considered_blocks.add(block)
        for block in considered_blocks:
            comparisons_without_closure = comparisons_without_closure + block_costs[block]
        print("Number of comparisons without transitive closure: " + str(comparisons_without_closure) + '\n',
              file=open(task.query_output, "a"))
        with open(task.query_details, 'a') as query_details:
            query_details.write(',' + str(comparisons_without_closure))
    end_computation = time.time() - start_computation
    print("computation of no-blocking comparisons: " + str(round(end_computation * 1000, 2)), file=open(log_file, "a"))

    with open(task.query_details, 'a') as query_details:
        query_details.write(',' + str(len(ol)))

    # Perform progressive entity resolution and count the number of comparisons before each emission

    # Number of comparisons
    count = 0
    previous_count = 0

    # List of emitted entities
    results = list()

    # Number of emitted entities (for Top-K query case)
    top_k_now = 0

    # At each iteration, order OL and check its first element (priority)
    while len(ol) > 0:
        # OL is always kept sorted on the ordering key (according to the specified ordering mode)
        start_sorting = time.time()
        # print("start OL sorting: " + str(start_sorting), file=open(log_file, "a"))
        if task.ordering_mode == 'asc':
            ol = sorted(ol, key=lambda x: x['ordering_key'] if not math.isnan(x['ordering_key']) else float('inf'))
        else:
            ol = sorted(ol, key=lambda x: x['ordering_key'] if not math.isnan(x['ordering_key']) else float('-inf'),
                        reverse=True)
        end_sorting = time.time() - start_sorting
        print("OL sorting: " + str(round(end_sorting * 1000, 2)), file=open(log_file, "a"))
        # If the first element of OL is already solved: perform ER, check HAVING clauses on it...
        # ...and eventually emit the entity
        start_check = time.time()
        # print("start priority check: " + str(start_check), file=open(log_file, "a"))
        if ol[0]['solved']:
            # Perform ER on the records represented by the element (identifiers)
            entity = resolution(ds, ol[0]['id'], task.aggregations)
            # Check HAVING clauses on the entity
            if task.brewer_post_filtering(entity):
                # Emit the entity keeping in memory the number of comparisons performed before its emission
                entity['comparisons'] = count
                # Keep also track of the time necessary for its emission
                timestamp = time.time()
                entity['time'] = timestamp - start_time
                results.append(entity)
                # Increment the emitted entities counter and check if it fits the (eventual) K value (Top-K query)...
                # ...if it is equal to K, return the result in advance
                top_k_now = top_k_now + 1
                if top_k_now == task.top_k:
                    return pd.DataFrame(results)
            # Remove the considered element from OL
            ol.pop(0)
        # If the first element of OL is not solved yet, find the matching neighbours...
        # ...and insert in OL a new element representing them
        else:
            original_key = ol[0]['ordering_key']
            # Of course, the first (and only guaranteed) matching element is the current record itself (already in 'id')
            matches = ol[0]['id']
            # Look for the matches in the neighbourhood
            if len(ol[0]['neighbours']['no_seed']) > 0:
                first_no_seed = ol[0]['neighbours']['no_seed'][0]
            else:
                first_no_seed = 'Ciao:)'
            neighbourhood = ol[0]['neighbours']['seed'] + ol[0]['neighbours']['no_seed']
            stop = False
            for n in neighbourhood:
                # If the record is not a seed record and does not match any seed record (Eager BrewER) it can be ignored
                if mode == 'eager' and n == first_no_seed and not ol[0]['seed']:
                    if len(matches) == 1:
                        stop = True
                        break
                # Do not compare with itself and with elements already inserted in a solved entity
                if n not in matches and n not in done:
                    # Increment the counter of comparisons
                    count = count + 1
                    # Application of the matching function
                    if (matches[0], n) in gold or (n, matches[0]) in gold:
                        matches.append(n)
            if not stop:
                # The ordering key of the new element is the aggregation of the ones of the matches
                key_aggregation = {task.ordering_key: task.aggregations[task.ordering_key]}
                entity = resolution(ds, matches, key_aggregation)
                # Define the new element of OL representing the group of matching elements
                solved = dict()
                solved['id'] = matches
                solved['ordering_key'] = float(entity[task.ordering_key])
                solved['solved'] = True
                # A solved element has no more neighbours
                solved['neighbours'] = list()
                solved['seed'] = True
                # Insert the matching elements in the list of solved records
                done = done + matches
                # Delete the matching elements from OL
                ol = [item for item in ol if item['id'][0] not in matches]
                # Insert in OL the new element representing them
                ol.append(solved)
            else:
                # Delete the current record from the ordering list and insert it in the list of solved records
                done = done + matches
                ol = [item for item in ol if item['id'][0] not in matches]
        end_check = time.time() - start_check
        print("check the first element: " + str(round(end_check * 1000, 2)), file=open(log_file, "a"))
        print("---comparisons count: " + str(count - previous_count), file=open(log_file, "a"))
        previous_count = count
    print("Total number of performed comparisons: " + str(count) + '\n', file=open(task.query_output, "a"))

    with open(task.query_details, 'a') as query_details:
        query_details.write(',' + str(count) + ',' + str(len(results)))

    return pd.DataFrame(results)


def main():
    # Acquire the requirements of the task to be performed
    for query_index in range(12, 14):
        task = td.FundingNoNanTask(query_index)

        # Save the query details in the apposite file
        if not os.path.isfile(task.query_details):
            with open(task.query_details, 'a') as query_details:
                query_details.write("index,ds_name,top_k,ok,ok_aggregation,ordering_mode,cond1,cond2,operator,"
                                    "no_closure,lazy_ol,lazy_tot,lazy_emitted,eager_ol,eager_tot,eager_emitted")
        with open(task.query_details, 'a') as query_details:
            query_details.write('\n' + str(task.counter) + ',' + task.ds_name + ',' + str(task.top_k) + ',' +
                                task.ordering_key + ',' + task.aggregations[task.ordering_key] + ',' +
                                task.ordering_mode + ',' + task.having[0][1] + ',' + task.having[1][1] + ',' +
                                task.operator)

        # Print the query
        print(task.query, file=open(task.query_output, "a"))

        # Load the dataset in DataFrame format, creating also an alternative version as a list of dictionaries
        ds = pd.read_csv(task.ds_path)
        ds[task.ordering_key] = pd.to_numeric(ds[task.ordering_key], errors='coerce')
        for column in ds.columns:
            if ds[column].dtype == 'object':
                ds[column] = ds[column].fillna('NaN')
        ds_dict = ds.to_dict('records')
        print("Number of records in the dataset: " + str(len(ds_dict)) + '\n', file=open(task.query_output, "a"))

        # Load the ground truth in DataFrame format and transform it into a set of tuples (matching pairs)
        gold = pd.read_csv(task.gold_path)
        gold = set(list(gold.itertuples(index=False, name=None)))
        print("Number of matching pairs in ground truth: " + str(len(gold)) + '\n', file=open(task.query_output, "a"))

        # Perform blocking on the dataset
        blocks, candidates, record_blocks, block_costs = blocking(task, ds, gold)

        # If required, perform batch ER on the candidate set to get the cleaned dataset (DataFrame composed by entities)
        # Then, perform the query on the clean dataset
        batch_results = pd.NA
        if task.batch:
            batch_entities = batch_er(task, ds, candidates, gold)
            # If 'ignore null' option is set, ignore the entities with null ordering key
            if task.ignore_null:
                batch_entities = batch_entities[batch_entities[task.ordering_key].notnull()]
            batch_results = task.batch_query(batch_entities)
            if len(batch_results.index) > 0:
                with pd.option_context('display.max_rows', None, 'display.max_columns', None):
                    print(batch_results, file=open(task.query_output, "a", encoding="utf-8"))
            else:
                print("No entities satisfied the query\n", file=open(task.query_output, "a"))

        # If in a discordant case, perform progressive ER through Lazy BrewER on the dataset
        brewer_attributes = task.attributes + ['comparisons', 'time']
        # if (task.aggregations[task.ordering_key] == 'max' and task.ordering_mode == 'asc') or \
        #         (task.aggregations[task.ordering_key] == 'min' and task.ordering_mode == 'desc'):
        if 1:
            lazy_results = brewer('lazy', task, ds, gold, blocks, record_blocks, block_costs)
            if len(lazy_results.index) > 0:
                with pd.option_context('display.max_rows', None, 'display.max_columns', None):
                    print(lazy_results.loc[:, brewer_attributes], file=open(task.query_output, "a", encoding="utf-8"))
                    lazy_results.loc[:, brewer_attributes].to_csv(task.lazy_output, index=False)
            else:
                print("No entities satisfied the query\n", file=open(task.query_output, "a"))

        # Perform progressive ER through Eager BrewER on the dataset
        eager_results = brewer('eager', task, ds, gold, blocks, record_blocks, block_costs)
        if len(eager_results.index) > 0:
            with pd.option_context('display.max_rows', None, 'display.max_columns', None):
                print(eager_results.loc[:, brewer_attributes], file=open(task.query_output, "a", encoding="utf-8"))
                eager_results.loc[:, brewer_attributes].to_csv(task.eager_output, index=False)
        else:
            print("No entities satisfied the query\n", file=open(task.query_output, "a"))

        if task.batch:
            # Verify that batch algorithm and BrewER produce the same entities
            if len(batch_results.index) > 0 and len(eager_results.index) > 0:
                batch_results = list(batch_results.fillna(0).to_records(index=False))
                eager_results = list(eager_results[task.attributes].fillna(0).to_records(index=False))
                print("\nDifferences in the produced entities between batch algorithm and BrewER: ",
                      file=open(task.query_output, "a"))
                # For Top-K query case, consider only the entities produced by BrewER (batch always exhaustive)
                if task.top_k > 0:
                    print([item for item in eager_results if item not in batch_results],
                          file=open(task.query_output, "a", encoding="utf-8"))
                else:
                    print(
                        [item for item in batch_results if item not in eager_results] + [item for item in eager_results
                                                                                         if item not in batch_results],
                        file=open(task.query_output, "a", encoding="utf-8"))
            else:
                print("One of the two algorithms did not return any entity\n", file=open(task.query_output, "a"))


if __name__ == "__main__":
    main()
